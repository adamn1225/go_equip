package main

import (
	"encoding/csv"
	"encoding/json"
	"fmt"
	"log"
	"os"
	"time"

	"github.com/adamn1225/hybrid-ocr-agent/scraper/internal/models"
	"github.com/adamn1225/hybrid-ocr-agent/scraper/ocrworker"
	"github.com/adamn1225/hybrid-ocr-agent/scraper/queue"
	"github.com/adamn1225/hybrid-ocr-agent/scraper/scraper"
)

// exportToCSV exports seller information to a CSV file
func exportToCSV(sellerInfos []map[string]string, filename string) error {
	file, err := os.Create(filename)
	if err != nil {
		return err
	}
	defer file.Close()

	writer := csv.NewWriter(file)
	defer writer.Flush()

	// Write header
	headers := []string{"Seller/Company", "Location", "Phone", "Email", "Serial Number", "Auction Date", "URL"}
	if err := writer.Write(headers); err != nil {
		return err
	}

	// Write data rows
	for _, info := range sellerInfos {
		row := []string{
			info["seller"],
			info["location"],
			info["phone"],
			info["email"],
			info["serial_number"],
			info["auction_date"],
			info["url"],
		}
		if err := writer.Write(row); err != nil {
			return err
		}
	}

	return nil
}

// exportToJSON exports seller information to a JSON file
func exportToJSON(sellerInfos []map[string]string, filename string, category string) error {
	// Create a structured format for better JSON output
	type Contact struct {
		Seller       string `json:"seller_company"`
		Location     string `json:"location"`
		Phone        string `json:"phone"`
		Email        string `json:"email,omitempty"`
		SerialNumber string `json:"serial_number,omitempty"`
		AuctionDate  string `json:"auction_date,omitempty"`
	}

	var contacts []Contact
	for _, info := range sellerInfos {
		contact := Contact{
			Seller:       info["seller"],
			Location:     info["location"],
			Phone:        info["phone"],
			Email:        info["email"],
			SerialNumber: info["serial_number"],
			AuctionDate:  info["auction_date"],
		}
		contacts = append(contacts, contact)
	}

	// Create export data with metadata
	exportData := map[string]interface{}{
		"export_timestamp":   time.Now().Format(time.RFC3339),
		"total_contacts":     len(contacts),
		"equipment_category": category,
		"source_site":        "machinerytrader.com",
		"contacts":           contacts,
	}

	jsonData, err := json.MarshalIndent(exportData, "", "  ")
	if err != nil {
		return err
	}

	return os.WriteFile(filename, jsonData, 0644)
}

func main() {
	// MachineryTrader category mapping
	categoryMap := map[string]string{
		"1060": "wheel loaders", // Agriculture
		"1015": "cranes",        // Mini Excavators
		"1025": "dozer",         // Dozers/Bulldozers
		"1026": "excavator",     // Excavators
		"1027": "loader",        // Loaders
		"1028": "scraper",       // Scrapers
		"1029": "grader",        // Graders
		// Add more as you discover them
	}

	// Dynamic page scraping - start from page 150 and continue until no more pages
	baseURL := "https://www.machinerytrader.com/listings/search?Category=1060&page=" //stopped at 258
	currentCategory := categoryMap["1060"]                                           // Extract category from URL
	startPage := 1                                                                   // Start from where you left off (was 190)
	maxPages := 400                                                                  // Total pages available
	maxConsecutiveFailures := 5                                                      // Stop if we get 5 consecutive failures (more robust)

	log.Printf("Starting dynamic multi-page OCR scraper from page %d", startPage)
	log.Printf("üéØ Scraping category: %s (Category=1060)", currentCategory)
	log.Printf("üéØ Target: Process through page %d (total %d pages)", maxPages, maxPages)
	log.Println("ü§ñ CAPTCHA solver integration enabled - will automatically handle CAPTCHAs")
	log.Println("üîç Starting Python CAPTCHA solver service...")

	// Start CAPTCHA solver service
	if err := scraper.StartCAPTCHASolverService(); err != nil {
		log.Printf("‚ö†Ô∏è  CAPTCHA solver not available: %v", err)
		log.Println("üîç Falling back to manual CAPTCHA handling - browser will be visible")
	} else {
		log.Println("‚úÖ CAPTCHA solver service is ready")
	}

	log.Println("üí° Will continue scraping until all pages are processed or consecutive failures occur...")

	var allSellerInfo []map[string]string
	consecutiveFailures := 0
	currentPage := startPage

	// Add defer to ensure data is saved even if script crashes
	defer func() {
		if len(allSellerInfo) > 0 {
			log.Printf("\nüíæ Emergency save triggered - saving %d contacts...", len(allSellerInfo))
			timestamp := time.Now().Format("20060102_150405")
			csvFile := fmt.Sprintf("seller_contacts_emergency_%s.csv", timestamp)
			jsonFile := fmt.Sprintf("seller_contacts_emergency_%s.json", timestamp)

			exportToCSV(allSellerInfo, csvFile)
			exportToJSON(allSellerInfo, jsonFile, currentCategory)
			log.Printf("üíæ Emergency data saved to %s and %s", csvFile, jsonFile)
		}
		scraper.CloseBrowserSession()
	}()

	for {
		targetURL := fmt.Sprintf("%s%d", baseURL, currentPage)

		log.Printf("\nüìÑ Processing page %d: %s", currentPage, targetURL)
		log.Printf("üîç DEBUG: startPage=%d, currentPage=%d, targetURL=%s", startPage, currentPage, targetURL)

		// Create job
		job := models.Job{URL: targetURL}

		// Take screenshot with CAPTCHA handling
		imagePath := scraper.TakeScreenshotPlaywrightWithCAPTCHA(job.URL)
		if imagePath == "" {
			log.Printf("‚ùå Failed to take screenshot for page %d (possibly no more pages or navigation issue)", currentPage)
			consecutiveFailures++
			if consecutiveFailures >= maxConsecutiveFailures {
				log.Printf("üõë Stopping: %d consecutive screenshot failures - likely reached end of available pages", maxConsecutiveFailures)
				break
			}
			currentPage++
			continue
		}

		// Reset consecutive failures on successful screenshot
		consecutiveFailures = 0

		// Update job with image path
		job.ImagePath = imagePath

		// Enqueue job for processing
		log.Printf("üì¶ Enqueueing job for page %d...", currentPage)
		if err := queue.Enqueue(job); err != nil {
			log.Printf("Error enqueueing job for page %d: %v", currentPage, err)
		}

		// Process OCR
		log.Printf("üîç Processing OCR for page %d...", currentPage)
		text, err := ocrworker.ExtractTextFromImage(imagePath)
		if err != nil {
			log.Printf("‚ùå OCR processing failed for page %d: %v", currentPage, err)
			log.Println("Note: Make sure tesseract-ocr is installed: sudo apt-get install tesseract-ocr")
		} else {
			// Extract seller information (now returns slice of contacts)
			sellerInfoList := ocrworker.ExtractSellerInfo(text, targetURL)
			if len(sellerInfoList) > 0 {
				log.Printf("‚úÖ Page %d: Found %d contacts", currentPage, len(sellerInfoList))
				allSellerInfo = append(allSellerInfo, sellerInfoList...)
				consecutiveFailures = 0 // Reset failures on successful extraction
			} else {
				log.Printf("‚ö†Ô∏è  Page %d: No contacts found (empty page or end of results)", currentPage)
				consecutiveFailures++
				if consecutiveFailures >= maxConsecutiveFailures {
					log.Printf("üõë Stopping: %d consecutive pages with no data - likely reached end of available listings", maxConsecutiveFailures)
					break
				}
			}
		}

		// Add delay between pages to be respectful
		if (currentPage-startPage+1)%5 == 0 {
			log.Printf("üìä Progress: Page %d | Total contacts: %d | Rate: %.1f contacts/page",
				currentPage, len(allSellerInfo), float64(len(allSellerInfo))/float64(currentPage-startPage+1))
		}
		time.Sleep(3 * time.Second)

		currentPage++

		// Optional: Add a reasonable upper limit to prevent infinite loops
		if currentPage > maxPages {
			log.Printf("üõë Reached maximum page limit (%d), stopping...", maxPages)
			break
		}
	}

	log.Printf("\nüéâ Multi-page scraping completed!")
	log.Printf("üìä Total pages processed: %d", currentPage-startPage)
	log.Printf("üìù Total seller contacts found: %d", len(allSellerInfo))

	if currentPage > maxPages {
		log.Printf("üîö Reason: Reached safety limit of %d pages", maxPages)
	} else if consecutiveFailures >= maxConsecutiveFailures {
		log.Printf("üîö Reason: %d consecutive failures - likely reached end of available pages", maxConsecutiveFailures)
	} else {
		log.Printf("üîö Reason: Manual termination or other exit condition")
	}

	if len(allSellerInfo) > 0 {
		avgContactsPerPage := float64(len(allSellerInfo)) / float64(currentPage-startPage)
		log.Printf("üìà Average contacts per page: %.1f", avgContactsPerPage)
	}

	if len(allSellerInfo) > 0 {
		// Export to CSV and JSON
		timestamp := time.Now().Format("20060102_150405")
		csvFile := fmt.Sprintf("seller_contacts_%s.csv", timestamp)
		jsonFile := fmt.Sprintf("seller_contacts_%s.json", timestamp)

		// Export to CSV
		err := exportToCSV(allSellerInfo, csvFile)
		if err != nil {
			log.Printf("‚ùå Error exporting to CSV: %v", err)
		} else {
			log.Printf("üíæ CSV exported successfully: %s", csvFile)
		}

		// Export to JSON
		err = exportToJSON(allSellerInfo, jsonFile, currentCategory)
		if err != nil {
			log.Printf("‚ùå Error exporting to JSON: %v", err)
		} else {
			log.Printf("üíæ JSON exported successfully: %s", jsonFile)
			log.Printf("üìä Category: %s, Site: machinerytrader.com", currentCategory)
		}
	}

	// Close the persistent browser session
	scraper.CloseBrowserSession()
}
