#!/usr/bin/env python3
"""
ğŸ”„ New Scraping Workflow with D1 Integration
This replaces the old JSON-based integration process

Step-by-Step Guide:
1. Run scraper (generates seller_contacts_*.json)
2. Run this script (uploads to D1) 
3. Dashboard automatically shows new data
"""

import os
import subprocess
import glob
import json
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

def run_command(command, description):
    """Run a command and show the result"""
    print(f"\\nğŸ”„ {description}")
    print(f"Command: {command}")
    
    try:
        result = subprocess.run(command, shell=True, capture_output=True, text=True)
        
        if result.returncode == 0:
            print("âœ… Success!")
            if result.stdout:
                print(result.stdout)
        else:
            print(f"âŒ Error: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"âŒ Failed to run command: {e}")
        return False
    
    return True

def main():
    print("ğŸš€ New Scraping â†’ D1 Integration Workflow")
    print("=" * 50)
    
    # Step 1: Check for new scraped files
    json_dir = os.path.join("mt_contacts", "json")
    patterns = [os.path.join(json_dir, "seller_contacts_*.json"), 
               os.path.join(json_dir, "seller_contacts_learning_*.json")]
    all_files = []
    for pattern in patterns:
        all_files.extend(glob.glob(pattern))
    
    if not all_files:
        print("âŒ No scraped data files found")
        print("ğŸ’¡ Make sure to run your Go scraper first:")
        print("   go run cmd/scraper/main.go --start-page 1 --end-page 50")
        return
    
    # Check processed files
    processed_file = "d1_processed_files.json"
    processed_files = []
    if os.path.exists(processed_file):
        with open(processed_file, 'r') as f:
            processed_files = json.load(f)
    
    new_files = [f for f in all_files if f not in processed_files]
    
    if not new_files:
        print("âœ… All files already processed")
    else:
        print(f"ğŸ“„ Found {len(new_files)} new files to process:")
        for file in new_files:
            print(f"   â€¢ {file}")
    
    # Step 2: Check D1 connection
    print("\\nğŸ” Checking D1 database connection...")
    
    required_env = ['CLOUDFLARE_ACCOUNT_ID', 'D1_DATABASE_ID', 'CLOUDFLARE_API_TOKEN']
    missing_env = [var for var in required_env if not os.getenv(var)]
    
    if missing_env:
        print(f"âŒ Missing environment variables: {missing_env}")
        print("ğŸ’¡ Make sure these are set in your .env file")
        return
    
    print("âœ… D1 credentials found")
    
    # Step 3: Run D1 integration
    if new_files:
        success = run_command("python ultra_fast_d1.py", "Uploading new data to D1 database")
        
        if success:
            print("\\nğŸ‰ Integration successful!")
        else:
            print("\\nâŒ Integration failed")
            return
    
    # Step 4: Show current database status
    print("\\nğŸ“Š Current database status:")
    run_command("python ultra_fast_d1.py status", "Getting D1 database statistics")
    
    # Step 5: Instructions for dashboard
    print("\\nğŸ¯ Next Steps:")
    print("1. ğŸ–¥ï¸  Start the dashboard:")
    print("   streamlit run dashboard.py")
    print("\\n2. ğŸ” New categories will be automatically available")
    print("\\n3. ğŸ“Š All scraped data is now in D1 database")
    print("\\n4. ğŸš€ Future scrapes: just run this script after scraping!")

if __name__ == "__main__":
    main()
